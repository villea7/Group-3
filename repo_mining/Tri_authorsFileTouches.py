import csv
import requests
from datetime import datetime, timezone

# =========================
# CONFIGURATION SECTION
# =========================

# Repository we are mining
REPO = "scottyab/rootbeer"

# This file contains only SOURCE FILES that was generated by the adapted script
INPUT_CSV = "data/file_rootbeer.csv"

# Output file that will be used later for the scatterplot
OUTPUT_CSV = "data/file_touches.csv"

# GitHub token
# TOKEN = "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" << real token
TOKEN = "ghp_qKnUCwXlpaKQ0dlxgWJ1NlB5JpwgDi3QeHmf"


# =========================
# HELPER FUNCTIONS
# =========================

def github_get(url):
    """
    Perform a GET request to the GitHub API using authentication.
    Returns parsed JSON response.
    """
    headers = {
        "Authorization": f"Bearer {TOKEN}",
        "Accept": "application/vnd.github+json",
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # fail fast if GitHub returns an error
    return response.json()

def parse_github_date(date_str):
    """
    Convert GitHub timestamp string into a timezone-aware datetime object.
    Example GitHub format: 2023-01-01T12:34:56Z
    """
    return datetime.strptime(date_str, "%Y-%m-%dT%H:%M:%SZ").replace(
        tzinfo=timezone.utc
    )


# =========================
# MAIN LOGIC
# =========================

def main():

    # ------------------------------------------------
    # 1) Read list of source files
    # ------------------------------------------------
    files = []
    with open(INPUT_CSV, newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            files.append(row["Filename"])

    # This list will store every (file, author, date) tuple
    results = []

    # Track earliest commit date to compute "week 0"
    earliest_date = None

    # ------------------------------------------------
    # 2) For EACH source file, query GitHub commits
    # ------------------------------------------------
    for filepath in files:

        page = 1              # GitHub paginates results
        more_pages = True     # lcv

        while more_pages:

            # Build GitHub API URL:
            # - path filters commits that touched this file
            # - per_page=100 is max allowed
            url = (
                f"https://api.github.com/repos/{REPO}/commits"
                f"?path={filepath}&per_page=100&page={page}"
            )

            commits = github_get(url)

            # If GitHub returns an empty list,
            # there are no more pages to process
            if len(commits) == 0:
                more_pages = False
                continue

            # ----------------------------------------
            # Process commits on this page
            # ----------------------------------------
            for commit_obj in commits:

                commit_info = commit_obj["commit"]

                # Commit date
                date_str = commit_info["author"]["date"]
                dt = parse_github_date(date_str)

                # Track earliest date for week calculation
                if earliest_date is None or dt < earliest_date:
                    earliest_date = dt

                # Author name:
                # Prefer GitHub username if available,
                # otherwise fall back to commit author name
                if commit_obj.get("author"):
                    author = commit_obj["author"]["login"]
                else:
                    author = commit_info["author"]["name"]

                results.append([filepath, author, date_str])

            # Move to next GitHub page
            page += 1

    # ------------------------------------------------
    # 3) Convert commit dates to "weeks since start"
    # ------------------------------------------------
    final_rows = []
    for filepath, author, date_str in results:

        dt = parse_github_date(date_str)

        # Week number relative to earliest commit observed
        weeks_since_start = (dt - earliest_date).days // 7

        final_rows.append([
            filepath,
            author,
            date_str,
            weeks_since_start
        ])

    # ------------------------------------------------
    # 4) Write output CSV
    # ------------------------------------------------
    with open(OUTPUT_CSV, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Filename", "Author", "DateUTC", "Week"])
        writer.writerows(final_rows)

    print(f"Wrote {len(final_rows)} rows to {OUTPUT_CSV}")


# Entry point
if __name__ == "__main__":
    main()
